{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from razdel import sentenize, tokenize\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c8cc9",
   "metadata": {},
   "source": [
    "<h2>Read raw data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d789ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "dir_path = \"/Users/igorpostoev/Projects/Postgraduate/author_classification_task/sources/\"\n",
    "\n",
    "def get_raw_text_from(path):\n",
    "    contents = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        filepath = path + filename\n",
    "        \n",
    "        # read input, remove redudant escape chars\n",
    "        with open(filepath) as f:\n",
    "            contents += [re.sub('[^А-Яа-я0-9 .,!?:;()]+,-', '', s) for s in f.readlines()]\n",
    "    contents = ' '.join(contents)\n",
    "    \n",
    "    return contents\n",
    "\n",
    "bulgakov_raw_data = get_raw_text_from(dir_path + \"bulgakov/\")\n",
    "gogol_raw_data = get_raw_text_from(dir_path + \"gogol/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a57829",
   "metadata": {},
   "source": [
    "<h2>Create dataframe</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a7c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datadarame(raw_text, sentenizer, label):\n",
    "    sentences = [s.text for s in sentenizer(raw_text)]\n",
    "    labels = [label for item in range(0, len(sentences))]\n",
    "    df = pd.DataFrame({\n",
    "        \"sentence\": sentences,\n",
    "        \"label\": labels,\n",
    "    })\n",
    "    df = df.sample(frac=1)\n",
    "    return df\n",
    "\n",
    "bul_df = create_datadarame(bulgakov_raw_data, sentenize, 0)\n",
    "gog_df = create_datadarame(gogol_raw_data, sentenize, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b0970",
   "metadata": {},
   "source": [
    "<h2>Text processing pipelines</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aeb316f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def cut_sentence(substrings, min_length, max_length):\n",
    "    return [t for t in substrings if len(t.text) > min_length and len(t.text) < max_length]\n",
    "\n",
    "def basic_token_pipeline(sentence, tokenizer):\n",
    "    tokenized = list(tokenizer(sentence))\n",
    "    return tokenized\n",
    "        \n",
    "def basic_label_pipeline(label):\n",
    "    return label\n",
    "\n",
    "def normalized_token_pipeline(sentence, tokenizer, translator, min_length=0, max_length=float(\"inf\")):\n",
    "    tokenized = list(tokenizer(sentence))\n",
    "    tokenized = [morph.parse(t.text)[0].normal_form for t in tokenized]\n",
    "    tokenized = cut_sentence(tokenized, min_length, max_length)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe139c",
   "metadata": {},
   "source": [
    "# make bow tensors by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cf62074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_counter(dfs, tokenizer):\n",
    "    sentences = [df[\"sentence\"] for df in dfs]\n",
    "    tokens = sum([[w.text for w in tokenizer(s)] for s in sentences], [])\n",
    "    counter = Counter(tokens)\n",
    "    return counter\n",
    "\n",
    "tokens_counter = create_counter([bul_df, gog_df], tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0da15aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "most_common_tokens = list(dict(c.most_common(vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c244052",
   "metadata": {},
   "outputs": [],
   "source": [
    "contained_matrix_bul = [[w in s for w in most_common_tokens] for s in tokenized_sentences_bul]\n",
    "contained_matrix_gog = [[w in s for w in most_common_tokens] for s in tokenized_sentences_gogol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c1e6b351",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bow_tensor_bul = torch.tensor(contained_matrix_bul, dtype=torch.float32)\n",
    "bow_tensor_gog = torch.tensor(contained_matrix_gog, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3feac88",
   "metadata": {},
   "source": [
    "# setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e9de37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4c281a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text_bow):\n",
    "        return self.fc(text_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModelWithEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text_bow):\n",
    "        return self.fc(text_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e4854d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bow = torch.tensor([x for x in df['X'].values], dtype=torch.float32)\n",
    "labels = torch.tensor([x for x in df['y'].values], dtype=torch.float32)\n",
    "num_class = len(set([label for label in labels.tolist()]))\n",
    "emsize = text_bow.shape[1]\n",
    "model = TextClassificationModel(emsize, num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adcca0",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "71b7a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 16 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "all_items = df.values\n",
    "num_validate = int(len(all_items) * 0.1)\n",
    "num_test = int(len(all_items) * 0.1)\n",
    "\n",
    "train_items = all_items[:len(all_items) - num_test - num_validate]\n",
    "test_items = all_items[len(train_items): len(train_items) + num_test]\n",
    "val_items = all_items[len(train_items) + num_test: len(all_items) + num_test + num_validate]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#split_train_, split_valid_ = \\\n",
    "   # random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        text_list.append(_text)\n",
    "        label_list.append(_label)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.float32)\n",
    "    return text_list.to(device), label_list.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_items, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_items, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_items, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fd47c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb0943dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.36s | valid accuracy    0.856 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.36s | valid accuracy    0.862 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.34s | valid accuracy    0.877 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.33s | valid accuracy    0.881 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.34s | valid accuracy    0.852 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.33s | valid accuracy    0.879 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.33s | valid accuracy    0.877 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.34s | valid accuracy    0.877 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.33s | valid accuracy    0.877 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.34s | valid accuracy    0.877 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "369e53d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3074, -0.1691],\n",
      "        [-0.3074, -0.1691],\n",
      "        [-0.3074, -0.1691],\n",
      "        [-0.0512, -0.1470],\n",
      "        [-0.3074, -0.1691]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(vocab_size, 2)\n",
    "#input = torch.tensor(bow_tensor_bul[0:1].clone().detach(), dtype=torch.float32)\n",
    "output = m(bow_tensor_bul[0:5])\n",
    "print(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9b3912c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0ed7ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['X']\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bcae2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = svm.SVC(C=1, decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db680b8d",
   "metadata": {},
   "source": [
    "# evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "547996ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: nan (nan)\n",
      "Recall: nan (nan)\n"
     ]
    }
   ],
   "source": [
    "metrics = cross_validate(model, X, y, scoring=['precision_macro', 'recall_macro'], cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Precision: %.3f (%.3f)' % (mean(metrics[\"test_precision_macro\"]), std(metrics[\"test_precision_macro\"])))\n",
    "print('Recall: %.3f (%.3f)' % (mean(metrics[\"test_recall_macro\"]), -std(metrics[\"test_recall_macro\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548fbf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45912, 45912])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "n = int(len(sentences) / 5)\n",
    " \n",
    "divided_sentences = list(divide_chunks(sentences, n))\n",
    "#frequencies\n",
    "counted_tokens = Counter(normalized_tokens).most_common()\n",
    "tokens_by_frequencies = dict([(w, n / len(counted_tokens)) for w, n in counted_tokens])\n",
    "freq_sentences_bul = [[tokens_by_frequencies[t] for t in s] for s in tokenized_sentences_bul]\n",
    "freq_sentences_gogol = [[tokens_by_frequencies[t] for t in s] for s in tokenized_sentences_gogol]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
