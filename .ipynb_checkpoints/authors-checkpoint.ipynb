{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fbb981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from razdel import sentenize, tokenize\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c8cc9",
   "metadata": {},
   "source": [
    "<h2>Read raw data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d789ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "dir_path = \"/Users/igorpostoev/Projects/Postgraduate/author_classification_task/sources/\"\n",
    "\n",
    "def get_raw_text_from(path):\n",
    "    contents = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        filepath = path + filename\n",
    "        \n",
    "        # read input, remove redudant escape chars\n",
    "        with open(filepath) as f:\n",
    "            contents += [re.sub('[^А-Яа-я0-9 .,!?:;()]+,-', '', s) for s in f.readlines()]\n",
    "    contents = ' '.join(contents)\n",
    "    \n",
    "    return contents\n",
    "\n",
    "bulgakov_raw_data = get_raw_text_from(dir_path + \"bulgakov/\")\n",
    "gogol_raw_data = get_raw_text_from(dir_path + \"gogol/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a57829",
   "metadata": {},
   "source": [
    "<h2>Create dataframe</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67a7c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datadarame(raw_text, sentenizer, label):\n",
    "    sentences = [s.text.replace('\\xa0', '').replace('\\n', '') for s in sentenizer(raw_text)]\n",
    "    labels = [label for item in range(0, len(sentences))]\n",
    "    df = pd.DataFrame({\n",
    "        \"sentence\": sentences,\n",
    "        \"label\": labels,\n",
    "    })\n",
    "    return df\n",
    "\n",
    "bul_df = create_datadarame(bulgakov_raw_data, sentenize, 0)\n",
    "gog_df = create_datadarame(gogol_raw_data, sentenize, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259daef5",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c101372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bul_df = bul_df.iloc[:len(bul_df) // 2]\n",
    "merged_df = pd.merge(bul_df, gog_df, how='outer')\n",
    "merged_df = merged_df.sample(frac=1)\n",
    "all_items = merged_df.values\n",
    "num_validate = int(len(all_items) * 0.1)\n",
    "num_test = int(len(all_items) * 0.1)\n",
    "\n",
    "train_items = all_items[:len(all_items)- num_test - num_validate]\n",
    "val_items = all_items[len(train_items): len(train_items) + num_validate]\n",
    "test_items = all_items[len(train_items) + num_test: len(train_items) + num_test + num_validate]\n",
    "\n",
    "#train_items = train_items[:len(train_items) // 2]\n",
    "#test_items = train_items[:len(test_items) // 2]\n",
    "#val_items = val_items[:len(val_items) // 2]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b0970",
   "metadata": {},
   "source": [
    "<h2>text processing pipelines</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb316f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def cut_sentence(substrings, min_length, max_length):\n",
    "    return [t for t in substrings if len(t) > min_length and len(t) < max_length]\n",
    "\n",
    "def normalize(sentence, tokenizer, min_length=0, max_length=float(\"inf\")):\n",
    "    tokenized = list(tokenizer(sentence))\n",
    "    tokenized = [morph.parse(t.text)[0].normal_form for t in tokenized]\n",
    "    tokenized = cut_sentence(tokenized, min_length, max_length)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4cf62074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW\n",
    "\n",
    "import nltk\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def ntlk_preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token.replace('\\n', ' ').replace(' ', '') for token in tokens]\n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "              # token not in russian_stopwords\\\n",
    "              #and token != \" \"]\n",
    "              #and token.strip() not in punctuation]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_counter(dfs, tokenizer):\n",
    "    sentences = [df[\"sentence\"].to_numpy().tolist() for df in dfs]\n",
    "    sentences = sum(sentences, [])\n",
    "    tokens = sum([[w for w in ntlk_preprocess_text(s)] for s in sentences], [])\n",
    "    counter = Counter(tokens)\n",
    "    return counter, len(tokens)\n",
    "\n",
    "def yield_bow_tokens(data_iter):\n",
    "    for _text, _ in data_iter:\n",
    "        yield iter([w for w in ntlk_preprocess_text(_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9bd3f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter, total = create_counter([bul_df, gog_df], tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "edc4ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "most_common_tokens = tokens_counter.most_common(vocab_size)\n",
    "token_to_tf = dict([(token, count / total) for (token, count) in most_common_tokens])\n",
    "bow_vocab = vocab(OrderedDict(most_common_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "07b0cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.zeros((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "aae96030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Отец любит свое дитя, мать любит свое дитя, дитя любит отца и мать.'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items[5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "594695e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184, 326, 32, 788, 0, 239, 326, 32, 788, 0, 788, 326, 184, 2, 239, 1]\n",
      "[0.0005369330384841784, 0.00030625069602430914, 0.00344830329159839, 0.00013522758006268197, 0.10190193614076395, 0.00042159186725424376, 0.00030625069602430914, 0.00344830329159839, 0.00013522758006268197, 0.10190193614076395, 0.00013522758006268197, 0.00030625069602430914, 0.0005369330384841784, 0.036348378064495604, 0.00042159186725424376, 0.04623590053613758]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10190194, 0.0462359 , 0.03634838, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_norm_pipeline(train_items[5,0], bow_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c3a9105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = np.zeros((1,3))\n",
    "bow_vector[0][1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8f907902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 0.]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0da15aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_pipeline(sentence):\n",
    "    tokenized_sentence =  list(tokenize(sentence))\n",
    "    return [token in tokenized_sentence for token in most_common_tokens]\n",
    "\n",
    "def bow_norm_pipeline(sentence, vocabulary):\n",
    "    tokenized_sentence_tfs = [(vocabulary[t], token_to_tf[t]) for t in ntlk_preprocess_text(sentence)]\n",
    "    indeces = [item[0] for item in tokenized_sentence_tfs]\n",
    "    tfs = [item[1] for item in tokenized_sentence_tfs]\n",
    "    bow_vector = np.zeros((1,len(vocabulary)))\n",
    "    bow_vector[0][indeces] = tfs\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b37c6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMB\n",
    "def yield_tokens(data_iter):\n",
    "    for _text, _ in data_iter:\n",
    "        yield iter([w.text for w in tokenize(_text)])\n",
    "        \n",
    "def yield_norm_tokens(data_iter):\n",
    "    for _text, _ in data_iter:\n",
    "        yield iter([w for w in normalize(_text, tokenize)])\n",
    "        \n",
    "def emb_text_pipeline(sentence):\n",
    "    return list(([vocab[w.text] for w in tokenize(sentence, tokenize)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5bdc51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(iter(train_items)), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def emb_text_pipeline(sentence):\n",
    "    return list(([vocab[w.text] for w in tokenize(sentence)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAVEC\n",
    "\n",
    "from navec import Navec\n",
    "import torch\n",
    "from slovnet.model.emb import NavecEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "navec = Navec.load('navec_hudlit_v1_12B_500K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navec_text_pipeline(sentence):\n",
    "    return list([navec[w] if w in navec else navec['<unk>'] for w in normalize(sentence, tokenize, min_length=2, max_length=25)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3feac88",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c281a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text_bow):\n",
    "        return self.fc(text_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModelEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModelEmbedding, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel2Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_class):\n",
    "        super(TextClassificationModel2Layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text_bow):\n",
    "        return self.fc2(self.fc1(text_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081abde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel2Layer(300, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adcca0",
   "metadata": {},
   "source": [
    "# prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e854c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "def collate_batch_emb(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(navec_text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), label_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "def collate_batch_navec(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(navec_text_pipeline(_text), dtype=torch.float32)\n",
    "        if len(processed_text) == 0:\n",
    "            processed_text = torch.tensor(navec_text_pipeline('<unk>')[0], dtype=torch.float32)\n",
    "        else:\n",
    "            processed_text = processed_text.sum(0)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list)\n",
    "    return text_list.to(device), label_list.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_items, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_navec)\n",
    "valid_dataloader = DataLoader(val_items, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_navec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04ec18",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0943dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# Hyperparameters\n",
    "EPOCHS = 4 # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "#split_train_, split_valid_ = \\\n",
    "   # random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d00cb8",
   "metadata": {},
   "source": [
    "<h2>Save results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_log(embedding_desc, model, learning_desc, validation):\n",
    "    date_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    data = [[embedding_desc, model.__str__(), learning_desc, validation, date_time]]\n",
    "    df = pandas.read_csv(dir_path + \"results.csv\")\n",
    "    df.loc[len(df.index)] = [''] + data[0]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(dir_path + \"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e53d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = nn.Linear(vocab_size, 2)\n",
    "#input = torch.tensor(bow_tensor_bul[0:1].clone().detach(), dtype=torch.float32)\n",
    "output = m(bow_tensor_bul[0:5])\n",
    "print(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb21a9",
   "metadata": {},
   "source": [
    "<h2>SVM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3912c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['X']\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = svm.SVC(C=1, decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db680b8d",
   "metadata": {},
   "source": [
    "# evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_norm_tokenize(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48207fbd",
   "metadata": {},
   "source": [
    "<h2>Embeddings option</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
