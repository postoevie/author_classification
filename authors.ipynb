{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fbb981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from razdel import sentenize, tokenize\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c8cc9",
   "metadata": {},
   "source": [
    "<h2>Read raw data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4d789ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "dir_path = \"/Users/igorpostoev/Projects/Postgraduate/author_classification_task/sources/\"\n",
    "\n",
    "def get_raw_text_from_dir(path):\n",
    "    contents = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        filepath = path + filename\n",
    "        \n",
    "        # read input, remove redudant escape chars\n",
    "        with open(filepath) as f:\n",
    "            contents += [re.sub('[^А-Яа-я0-9 .,!?:;()]+,-', '', s) for s in f.readlines()]\n",
    "    contents = ' '.join(contents)\n",
    "    \n",
    "    return contents\n",
    "\n",
    "def get_raw_text_from(path):\n",
    "    contents = []\n",
    "    if not path.endswith(\".txt\"):\n",
    "        return contents\n",
    "        # read input, remove redudant escape chars\n",
    "    with open(path) as f:\n",
    "        contents += [re.sub('[^А-Яа-я0-9 .,!?:;()]+,-', '', s) for s in f.readlines()]\n",
    "    contents = ' '.join(contents)\n",
    "    return contents\n",
    "\n",
    "bulgakov_raw_data = get_raw_text_from_dir(dir_path + \"bulgakov/\")\n",
    "gogol_raw_data = get_raw_text_from_dir(dir_path + \"gogol/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259daef5",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3156329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datadarame(raw_text, sentenizer, label):\n",
    "    sentences = [s.text.replace('\\xa0', '').replace('\\n', '') for s in sentenizer(raw_text)]\n",
    "    labels = [label for item in range(0, len(sentences))]\n",
    "    df = pd.DataFrame({\n",
    "        \"sentence\": sentences,\n",
    "        \"label\": labels,\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c101372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bul_df = create_datadarame(bulgakov_raw_data, sentenize, 0)\n",
    "gog_df = create_datadarame(gogol_raw_data, sentenize, 1)\n",
    "bul_df = bul_df.iloc[:int(len(bul_df) * 0.7)]\n",
    "merged_df = pd.merge(bul_df, gog_df, how='outer')\n",
    "#shuffle here\n",
    "merged_df = merged_df.sample(frac=1)\n",
    "all_items = merged_df.values\n",
    "num_validate = int(len(all_items) * 0.2)\n",
    "num_test = int(len(all_items) * 0.1)\n",
    "\n",
    "train_items = all_items[:len(all_items)- num_test - num_validate]\n",
    "val_items = all_items[len(train_items): len(train_items) + num_validate]\n",
    "test_items = all_items[len(train_items) + num_test: len(train_items) + num_test + num_validate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014742a",
   "metadata": {},
   "source": [
    "# text metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(list(tokenize(sentence))) for sentence in bul_df[\"sentence\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(list(tokenize(sentence))) for sentence in gog_df[\"sentence\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bul_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6811985e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.099675450218777"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array([len(w.text) for w in list(tokenize(sentence))]).mean() for sentence in gog_df[\"sentence\"]]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9163a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array([np.array([w.text for w in list(tokenize(sentence))]) for sentence in bul_df[\"sentence\"]], dtype=object)\n",
    "tokens = np.concatenate(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "dist = FreqDist(tokens)\n",
    "toks = [pair[0] for pair in dist.most_common(30)]\n",
    "nums = [pair[1] for pair in dist.most_common(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "551561ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/85/y194gy196m5gh1smpgnk_7w80000gn/T/ipykernel_61582/655228692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "names = ['group_a', 'group_b', 'group_c']\n",
    "values = [1, 10, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10), dpi=100)\n",
    "\n",
    "plt.bar(toks, nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b0970",
   "metadata": {},
   "source": [
    "<h2>text processing pipelines</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb316f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def cut_sentence(substrings, min_length, max_length):\n",
    "    return [t for t in substrings if len(t) > min_length and len(t) < max_length]\n",
    "\n",
    "def normalize(sentence, tokenizer, min_length=0, max_length=float(\"inf\")):\n",
    "    tokenized = list(tokenizer(sentence))\n",
    "    tokenized = [morph.parse(t.text)[0].normal_form for t in tokenized]\n",
    "    tokenized = cut_sentence(tokenized, min_length, max_length)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf62074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW\n",
    "\n",
    "import nltk\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def ntlk_preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token.replace('\\n', ' ').replace(' ', '') for token in tokens]\n",
    "    tokens = [token for token in tokens if len(token) > 0\n",
    "              # token not in russian_stopwords\\\n",
    "              #and token != \" \"]\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_counter(dfs, tokenizer):\n",
    "    sentences = [df[\"sentence\"].to_numpy().tolist() for df in dfs]\n",
    "    sentences = sum(sentences, [])\n",
    "    tokens = sum([[w for w in ntlk_preprocess_text(s)] for s in sentences], [])\n",
    "    counter = Counter(tokens)\n",
    "    return counter, len(tokens)\n",
    "\n",
    "def yield_bow_tokens(data_iter):\n",
    "    for _text, _ in data_iter:\n",
    "        yield iter([w for w in ntlk_preprocess_text(_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c9aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter, total = create_counter([bul_df, gog_df], tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d278a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4000\n",
    "most_common_tokens = tokens_counter.most_common(vocab_size)\n",
    "token_to_tf = dict([(token, count / total) for (token, count) in most_common_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "135ac867",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vocab = vocab(OrderedDict(most_common_tokens), specials=[\"<unk>\"])\n",
    "bow_vocab.set_default_index(bow_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da15aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_pipeline(sentence):\n",
    "    tokenized_sentence =  list(tokenize(sentence))\n",
    "    return [token in tokenized_sentence for token in most_common_tokens]\n",
    "\n",
    "def bow_norm_pipeline(sentence, vocabulary):\n",
    "    tokenized_sentence_tfs = [(vocabulary[t], token_to_tf[t] if t in token_to_tf.keys() else 0) for t in ntlk_preprocess_text(sentence)]\n",
    "    indeces = [item[0] for item in tokenized_sentence_tfs]\n",
    "    tfs = [1 for item in tokenized_sentence_tfs]\n",
    "    bow_vector = np.zeros((1,len(vocabulary)))\n",
    "    bow_vector[0][indeces] = tfs\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b37c6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMB\n",
    "def yield_tokens(data_iter):\n",
    "    for _text, _ in data_iter:\n",
    "        yield iter([w.text for w in tokenize(_text)])\n",
    "        \n",
    "def yield_norm_tokens(data_iter):\n",
    "    for _text, _ in data_iter:\n",
    "        yield iter([w for w in normalize(_text, tokenize)])\n",
    "\n",
    "def emb_text_pipeline(sentence):\n",
    "    return list(([bow_vocab[w.text] for w in tokenize(sentence)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd56ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAVEC\n",
    "\n",
    "from navec import Navec\n",
    "import torch\n",
    "from slovnet.model.emb import NavecEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04d5d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "navec = Navec.load('navec_hudlit_v1_12B_500K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72d8a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navec_text_pipeline(sentence):\n",
    "    return list([navec[w] if w in navec else navec['<unk>'] for w in ntlk_preprocess_text(sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3feac88",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c4c281a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text_bow):\n",
    "        return self.fc(text_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23f27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModelEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModelEmbedding, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel2Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_class):\n",
    "        super(TextClassificationModel2Layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, 10000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10000, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text_bow):\n",
    "        fc1_out = self.fc1(text_bow)\n",
    "        relu_out = self.relu(fc1_out)\n",
    "        return self.fc2(relu_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "081abde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(300, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adcca0",
   "metadata": {},
   "source": [
    "# prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e854c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def collate_batch_bow(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(bow_norm_pipeline(_text, bow_vocab), dtype=torch.float32)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list).sum(1)\n",
    "    return text_list.to(device), label_list.to(device)\n",
    "\n",
    "def collate_batch_emb(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(emb_text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list.to(device), label_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "def collate_batch_navec(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(navec_text_pipeline(_text), dtype=torch.float32)\n",
    "        if len(processed_text) == 0:\n",
    "            processed_text = torch.tensor(navec_text_pipeline('<unk>')[0], dtype=torch.float32)\n",
    "        else:\n",
    "            processed_text = processed_text.sum(0)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list)\n",
    "    return text_list.to(device), label_list.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_items, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_navec)\n",
    "valid_dataloader = DataLoader(val_items, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d555cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3aae1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Лишь только доктор повернулся, изумление выросло в глазах преследователя, и доктору показалось, что это монгольские раскосые глаза.',\n",
       "  0)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,y) for (x,y) in train_items[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3b2560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch_navec(train_items[:2])[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04ec18",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd47c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cb0943dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1007 batches | accuracy    0.660\n",
      "| epoch   1 |  1000/ 1007 batches | accuracy    0.664\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 19.96s | valid accuracy    0.656 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1007 batches | accuracy    0.669\n",
      "| epoch   2 |  1000/ 1007 batches | accuracy    0.670\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 17.89s | valid accuracy    0.662 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1007 batches | accuracy    0.671\n",
      "| epoch   3 |  1000/ 1007 batches | accuracy    0.670\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 18.20s | valid accuracy    0.662 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1007 batches | accuracy    0.668\n",
      "| epoch   4 |  1000/ 1007 batches | accuracy    0.684\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 18.12s | valid accuracy    0.665 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1007 batches | accuracy    0.678\n",
      "| epoch   5 |  1000/ 1007 batches | accuracy    0.681\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 17.49s | valid accuracy    0.667 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1007 batches | accuracy    0.680\n",
      "| epoch   6 |  1000/ 1007 batches | accuracy    0.685\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 16.37s | valid accuracy    0.671 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1007 batches | accuracy    0.685\n",
      "| epoch   7 |  1000/ 1007 batches | accuracy    0.690\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 17.82s | valid accuracy    0.673 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1007 batches | accuracy    0.688\n",
      "| epoch   8 |  1000/ 1007 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 16.37s | valid accuracy    0.677 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1007 batches | accuracy    0.688\n",
      "| epoch   9 |  1000/ 1007 batches | accuracy    0.698\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 16.33s | valid accuracy    0.682 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1007 batches | accuracy    0.693\n",
      "| epoch  10 |  1000/ 1007 batches | accuracy    0.697\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 16.36s | valid accuracy    0.680 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   500/ 1007 batches | accuracy    0.703\n",
      "| epoch  11 |  1000/ 1007 batches | accuracy    0.691\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time: 16.33s | valid accuracy    0.682 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   500/ 1007 batches | accuracy    0.696\n",
      "| epoch  12 |  1000/ 1007 batches | accuracy    0.699\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time: 16.33s | valid accuracy    0.683 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   500/ 1007 batches | accuracy    0.698\n",
      "| epoch  13 |  1000/ 1007 batches | accuracy    0.697\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time: 17.53s | valid accuracy    0.685 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   500/ 1007 batches | accuracy    0.696\n",
      "| epoch  14 |  1000/ 1007 batches | accuracy    0.700\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time: 16.93s | valid accuracy    0.685 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   500/ 1007 batches | accuracy    0.696\n",
      "| epoch  15 |  1000/ 1007 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time: 16.89s | valid accuracy    0.685 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   500/ 1007 batches | accuracy    0.697\n",
      "| epoch  16 |  1000/ 1007 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time: 16.65s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   500/ 1007 batches | accuracy    0.701\n",
      "| epoch  17 |  1000/ 1007 batches | accuracy    0.696\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time: 17.08s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   500/ 1007 batches | accuracy    0.693\n",
      "| epoch  18 |  1000/ 1007 batches | accuracy    0.704\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time: 16.45s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n",
      "| epoch  19 |   500/ 1007 batches | accuracy    0.698\n",
      "| epoch  19 |  1000/ 1007 batches | accuracy    0.699\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time: 17.66s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n",
      "| epoch  20 |   500/ 1007 batches | accuracy    0.699\n",
      "| epoch  20 |  1000/ 1007 batches | accuracy    0.698\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time: 18.17s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# Hyperparameters\n",
    "EPOCHS = 20  # epoch\n",
    "LR = 10e-3#5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "#split_train_, split_valid_ = \\\n",
    "   # random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d00cb8",
   "metadata": {},
   "source": [
    "<h2>Save results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_log(embedding_desc, model, learning_desc, validation):\n",
    "    date_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    data = [[embedding_desc, model.__str__(), learning_desc, validation, date_time]]\n",
    "    df = pandas.read_csv(dir_path + \"results.csv\")\n",
    "    df.loc[len(df.index)] = [''] + data[0]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(dir_path + \"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e53d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = nn.Linear(vocab_size, 2)\n",
    "#input = torch.tensor(bow_tensor_bul[0:1].clone().detach(), dtype=torch.float32)\n",
    "output = m(bow_tensor_bul[0:5])\n",
    "print(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ffc2b",
   "metadata": {},
   "source": [
    "# Reslut plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f005c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal = predicted[:100].argmax(1).numpy() != np.array(true[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1114f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(model(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f35d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_to_eval = train_items[:1000]\n",
    "col = collate_batch_bow(items_to_eval)\n",
    "true = [item[1] for item in items_to_eval]\n",
    "softmax = nn.Softmax(dim=1)\n",
    "result = model(col[0])\n",
    "predicted = softmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab86bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_draw = torch.tensor([p[0] - p[1] if t == 0 else p[1] - p[0] for t, p in zip(true, predicted)])\n",
    "data_to_draw = data_to_draw.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036563e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(100, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rang = range(len(data_to_draw))\n",
    "negative_data_min = [x if x < 0 and x >= -0.5 else 0 for x in data_to_draw]\n",
    "negative_data_max = [x if x < -0.5 else 0 for x in data_to_draw]\n",
    "positive_data_min = [x if x > 0 and x <= 0.5 else 0 for x in data_to_draw]\n",
    "positive_data_max = [x if x > 0.5 else 0 for x in data_to_draw]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15), dpi=100)\n",
    "plt.xticks(np.arange(len(data_to_draw), step=50))\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(rang, negative_data_min, width=1, color='r', alpha=0.4)\n",
    "ax.bar(rang, negative_data_max, width=1, color='r', alpha=0.8)\n",
    "ax.bar(rang, positive_data_min, width=1, color='g', alpha=0.4)\n",
    "ax.bar(rang, positive_data_max, width=1, color='g', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ac358",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 100, 15\n",
    "x = mu + sigma * np.random.randn(10000)\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 6, density=True, facecolor='g', alpha=1)\n",
    "\n",
    "plt.xlabel('Inputs')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('BoW + FF 1')\n",
    "#plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.axis([30, 500, -2, 2])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gog_text = \"Не только в вашей стране, но и в других странах будут сочинять подражания его пьесам и писать переделки этих пьес. Ученые различных стран напишут подробные исследования его произведений и шаг за шагом постараются проследить его таинственную жизнь. Они докажут вам, что этот человек, который сейчас у вас в руках подает лишь слабые признаки жизни, будет влиять на многих писателей будущих столетий, в том числе на таких, неизвестных вам, но известных мне, как соотечественники мои Грибоедов, Пушкин и Гоголь.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gog_input = collate_batch_bow([(gog_text, 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(gog_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3876cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb21a9",
   "metadata": {},
   "source": [
    "<h2>SVM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3912c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['X']\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = svm.SVC(C=1, decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db680b8d",
   "metadata": {},
   "source": [
    "# evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_norm_tokenize(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48207fbd",
   "metadata": {},
   "source": [
    "<h2>Embeddings option</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
